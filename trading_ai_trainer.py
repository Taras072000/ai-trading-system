#!/usr/bin/env python3
"""
Trading AI Trainer - –°–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è —Ç–æ—Ä–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏
–¶–µ–ª—å: –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ –≤–∏–Ω—Ä–µ–π—Ç–∞ 75%+

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python trading_ai_trainer.py --symbol BTCUSDT --days 365 --train
    python trading_ai_trainer.py --symbol BTCUSDT --backtest
"""

import asyncio
import argparse
import logging
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import joblib
import json
import os
from pathlib import Path

# ML –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.utils.class_weight import compute_class_weight
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
from sklearn.inspection import permutation_importance
import xgboost as xgb
import lightgbm as lgb

# –õ–æ–∫–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã
from enhanced_indicators import EnhancedTechnicalIndicators
from ai_modules.trading_ai import TradingSignal
import config
from data_collector import BinanceDataCollector, DataManager
from improved_labeling_strategy import ImprovedLabelingStrategy

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('trading_ai_trainer.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class TradingAITrainer:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ç–æ—Ä–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏"""
    
    def __init__(self, symbol: str = "BTCUSDT"):
        self.symbol = symbol
        self.indicators = EnhancedTechnicalIndicators()
        self.models = {}
        self.scalers = {}
        self.feature_names = []
        self.model_dir = Path("models/trading_ai")
        self.model_dir.mkdir(parents=True, exist_ok=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
        self.data_manager = DataManager()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Ä–∞–∑–º–µ—Ç–∫–∏
        self.labeling_strategy = ImprovedLabelingStrategy({
            'prediction_horizon': 1,
            'target_class_balance': 0.3,
            'min_return_threshold': 0.002,
            'volatility_multiplier': 1.5,
            'use_percentile_thresholds': True,
            'buy_percentile': 70,  # –ë–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–µ –ø–æ—Ä–æ–≥–∏
            'sell_percentile': 30,
            'momentum_weight': 0.3,
            'volume_weight': 0.2,
        })
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
        self.config = {
            'lookback_period': 50,  # –ü–µ—Ä–∏–æ–¥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            'prediction_horizon': 1,  # –ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞ (1 = —Å–ª–µ–¥—É—é—â–∞—è —Å–≤–µ—á–∞)
            'min_confidence': 0.6,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —Å–∏–≥–Ω–∞–ª–∞
            'target_winrate': 0.75,  # –¶–µ–ª–µ–≤–æ–π –≤–∏–Ω—Ä–µ–π—Ç
            'risk_reward_ratio': 2.0,  # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ä–∏—Å–∫/–ø—Ä–∏–±—ã–ª—å
            'stop_loss_pct': 0.005,  # –°—Ç–æ–ø-–ª–æ—Å—Å –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö (—Å–Ω–∏–∂–µ–Ω–æ —Å 0.02 –¥–æ 0.005)
            'take_profit_pct': 0.01,  # –¢–µ–π–∫-–ø—Ä–æ—Ñ–∏—Ç –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö (—Å–Ω–∏–∂–µ–Ω–æ —Å 0.04 –¥–æ 0.01)
            'use_improved_labeling': True,  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —É–ª—É—á—à–µ–Ω–Ω—É—é —Ä–∞–∑–º–µ—Ç–∫—É
            'use_class_weights': True,  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
        }
    
    async def load_market_data(self, days: int = 365) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å Binance API"""
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {self.symbol} –∑–∞ {days} –¥–Ω–µ–π")
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º DataManager –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            data = await self.data_manager.ensure_data_available(
                symbol=self.symbol,
                interval="1h",  # –ß–∞—Å–æ–≤—ã–µ —Å–≤–µ—á–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
                days=days,
                force_update=False  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ —Å–≤–µ–∂–∏–µ
            )
            
            if data is None or len(data) == 0:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è {self.symbol}")
                return None
            
            # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏
            data = data.sort_values('timestamp').reset_index(drop=True)
            
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è {self.symbol}")
            logger.info(f"–ü–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö: {data['timestamp'].min()} - {data['timestamp'].max()}")
            
            return data
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {self.symbol}: {str(e)}")
            # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None, —á—Ç–æ–±—ã –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤ –≤—ã–∑—ã–≤–∞—é—â–µ–º –∫–æ–¥–µ
            return None
    
    def prepare_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        logger.info("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        features_list = []
        
        for i in range(self.config['lookback_period'], len(data)):
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –æ–∫–Ω–∞
            window_data = data.iloc[i-self.config['lookback_period']:i].copy()
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            indicators = self.indicators.calculate_all_indicators(window_data)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            current_time = data.iloc[i]['timestamp']
            indicators['hour'] = current_time.hour
            indicators['day_of_week'] = current_time.weekday()
            indicators['day_of_month'] = current_time.day
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ü–µ–Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            current_price = data.iloc[i]['close']
            prev_price = data.iloc[i-1]['close']
            indicators['price_change'] = (current_price - prev_price) / prev_price
            indicators['current_price'] = current_price
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω–¥–µ–∫—Å –¥–ª—è —Å–≤—è–∑–∏ —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
            indicators['index'] = i
            
            features_list.append(indicators)
        
        features_df = pd.DataFrame(features_list)
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN
        features_df = features_df.dropna()
        
        logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(features_df)} –æ–±—Ä–∞–∑—Ü–æ–≤ —Å {len(features_df.columns)} –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏")
        return features_df
    
    def balance_indicators(self, X: pd.DataFrame, y: np.ndarray, top_k_features: int = 50) -> Tuple[pd.DataFrame, List[str]]:
        """
        üéØ –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–ê –ò–ù–î–ò–ö–ê–¢–û–†–û–í - –æ—Ç–±–æ—Ä –Ω–∞–∏–±–æ–ª–µ–µ –∑–Ω–∞—á–∏–º—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        –¶–µ–ª—å: –ü–æ–≤—ã—Å–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –æ—Ç—Å–µ–∏–≤–∞—è —Å–ª–∞–±—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        """
        logger.info(f"üîç –ù–∞—á–∏–Ω–∞—é –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ (–æ—Ç–±–æ—Ä {top_k_features} –ª—É—á—à–∏—Ö –∏–∑ {len(X.columns)})")
        
        # –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        feature_cols = [col for col in X.columns if col not in ['index', 'current_price']]
        X_features = X[feature_cols].copy()
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è –º–µ–¥–∏–∞–Ω–æ–π
        X_features = X_features.fillna(X_features.median())
        
        # 1. –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó - F-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        logger.info("üìä –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ (F-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞)...")
        f_selector = SelectKBest(score_func=f_classif, k=min(top_k_features * 2, len(feature_cols)))
        X_f_selected = f_selector.fit_transform(X_features, y)
        f_scores = f_selector.scores_
        f_selected_features = [feature_cols[i] for i in f_selector.get_support(indices=True)]
        
        # 2. –ò–ù–§–û–†–ú–ê–¶–ò–û–ù–ù–´–ô –ê–ù–ê–õ–ò–ó - Mutual Information
        logger.info("üß† –ê–Ω–∞–ª–∏–∑ –≤–∑–∞–∏–º–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (Mutual Information)...")
        mi_selector = SelectKBest(score_func=mutual_info_classif, k=min(top_k_features * 2, len(feature_cols)))
        X_mi_selected = mi_selector.fit_transform(X_features, y)
        mi_scores = mi_selector.scores_
        mi_selected_features = [feature_cols[i] for i in mi_selector.get_support(indices=True)]
        
        # 3. –ú–û–î–ï–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó - Feature Importance —Å Random Forest
        logger.info("üå≤ –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (Random Forest)...")
        rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
        rf_model.fit(X_features, y)
        rf_importance = rf_model.feature_importances_
        
        # 4. –ö–û–ú–ë–ò–ù–ò–†–û–í–ê–ù–ù–´–ô –°–ö–û–†
        logger.info("‚öñÔ∏è –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–∫–æ—Ä–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏...")
        feature_scores = {}
        
        for i, feature in enumerate(feature_cols):
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å–∫–æ—Ä—ã –∫ –¥–∏–∞–ø–∞–∑–æ–Ω—É [0, 1]
            f_score_norm = f_scores[i] / (np.max(f_scores) + 1e-8) if i < len(f_scores) else 0
            mi_score_norm = mi_scores[i] / (np.max(mi_scores) + 1e-8) if i < len(mi_scores) else 0
            rf_score_norm = rf_importance[i] / (np.max(rf_importance) + 1e-8)
            
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫–æ—Ä: 30% F-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ + 30% MI + 40% RF –≤–∞–∂–Ω–æ—Å—Ç—å
            combined_score = (f_score_norm * 0.3) + (mi_score_norm * 0.3) + (rf_score_norm * 0.4)
            feature_scores[feature] = {
                'combined_score': combined_score,
                'f_score': f_score_norm,
                'mi_score': mi_score_norm,
                'rf_importance': rf_score_norm
            }
        
        # 5. –û–¢–ë–û–† –õ–£–ß–®–ò–• –ü–†–ò–ó–ù–ê–ö–û–í
        sorted_features = sorted(feature_scores.items(), key=lambda x: x[1]['combined_score'], reverse=True)
        selected_features = [feature for feature, _ in sorted_features[:top_k_features]]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        final_features = selected_features + ['index', 'current_price']
        X_balanced = X[final_features].copy()
        
        # 6. –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
        logger.info(f"‚úÖ –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –û—Ç–æ–±—Ä–∞–Ω–æ {len(selected_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ {len(feature_cols)}")
        logger.info(f"üèÜ –¢–û–ü-10 –ª—É—á—à–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤:")
        for i, (feature, scores) in enumerate(sorted_features[:10]):
            logger.info(f"   {i+1:2d}. {feature:25s} - —Å–∫–æ—Ä: {scores['combined_score']:.4f} "
                       f"(F:{scores['f_score']:.3f}, MI:{scores['mi_score']:.3f}, RF:{scores['rf_importance']:.3f})")
        
        logger.info(f"‚ùå –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ª–∞–±—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã ({len(feature_cols) - len(selected_features)}):")
        weak_features = [feature for feature, _ in sorted_features[top_k_features:]]
        for i, (feature, scores) in enumerate(sorted_features[top_k_features:top_k_features+5]):
            logger.info(f"   {feature:25s} - —Å–∫–æ—Ä: {scores['combined_score']:.4f}")
        if len(weak_features) > 5:
            logger.info(f"   ... –∏ –µ—â—ë {len(weak_features) - 5} —Å–ª–∞–±—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        
        # 7. –°–û–•–†–ê–ù–ï–ù–ò–ï –ò–ù–§–û–†–ú–ê–¶–ò–ò –û –í–ê–ñ–ù–û–°–¢–ò –ü–†–ò–ó–ù–ê–ö–û–í
        self.feature_importance_analysis = {
            'selected_features': selected_features,
            'feature_scores': feature_scores,
            'selection_method': 'combined_f_mi_rf',
            'top_k': top_k_features,
            'total_features_before': len(feature_cols),
            'total_features_after': len(selected_features)
        }
        
        return X_balanced, selected_features
    
    def analyze_feature_quality(self, X: pd.DataFrame, y: np.ndarray, selected_features: List[str]) -> Dict:
        """
        üìà –ê–ù–ê–õ–ò–ó –ö–ê–ß–ï–°–¢–í–ê –û–¢–û–ë–†–ê–ù–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ä–∞–∑–¥–µ–ª—è—é—Ç –∫–ª–∞—Å—Å—ã
        """
        logger.info("üî¨ –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        feature_cols = [col for col in selected_features if col not in ['index', 'current_price']]
        X_features = X[feature_cols].fillna(X[feature_cols].median())
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–¥–µ–ª–∏–º–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤
        class_separation = {}
        for feature in feature_cols:
            feature_values = X_features[feature].values
            class_means = {}
            class_stds = {}
            
            for class_label in np.unique(y):
                class_mask = (y == class_label)
                class_values = feature_values[class_mask]
                class_means[class_label] = np.mean(class_values)
                class_stds[class_label] = np.std(class_values)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Ä–∞–∑–¥–µ–ª–∏–º–æ—Å—Ç–∏ (–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –º–µ–∂–∫–ª–∞—Å—Å–æ–≤–æ–≥–æ –∫ –≤–Ω—É—Ç—Ä–∏–∫–ª–∞—Å—Å–æ–≤–æ–º—É —Ä–∞–∑–±—Ä–æ—Å—É)
            between_class_var = np.var(list(class_means.values()))
            within_class_var = np.mean(list(class_stds.values())) ** 2
            separation_ratio = between_class_var / (within_class_var + 1e-8)
            
            class_separation[feature] = {
                'separation_ratio': separation_ratio,
                'class_means': class_means,
                'class_stds': class_stds
            }
        
        # –¢–æ–ø –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ —Ä–∞–∑–¥–µ–ª–∏–º–æ—Å—Ç–∏
        top_separating_features = sorted(class_separation.items(), 
                                       key=lambda x: x[1]['separation_ratio'], 
                                       reverse=True)[:10]
        
        logger.info("üéØ –¢–û–ü-10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ —Ä–∞–∑–¥–µ–ª–∏–º–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤:")
        for i, (feature, data) in enumerate(top_separating_features):
            logger.info(f"   {i+1:2d}. {feature:25s} - —Ä–∞–∑–¥–µ–ª–∏–º–æ—Å—Ç—å: {data['separation_ratio']:.4f}")
        
        quality_analysis = {
            'class_separation': class_separation,
            'top_separating_features': [f[0] for f in top_separating_features],
            'average_separation_ratio': np.mean([data['separation_ratio'] for data in class_separation.values()]),
            'feature_count': len(feature_cols)
        }
        
        return quality_analysis
    
    def create_labels(self, data: pd.DataFrame, features_df: pd.DataFrame) -> Tuple[np.ndarray, pd.DataFrame]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç–æ–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""
        logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç–æ–∫...")
        
        if self.config.get('use_improved_labeling', True):
            logger.info("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ä–∞–∑–º–µ—Ç–∫–∏")
            labels, filtered_features_df = self.labeling_strategy.create_enhanced_labels(features_df)
            return labels, filtered_features_df
        else:
            # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Ä–∞–∑–º–µ—Ç–∫–∏ (–¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)
            logger.info("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ä–∞–∑–º–µ—Ç–∫–∏")
            labels = []
            returns = []
            
            for idx in features_df['index']:
                if idx + self.config['prediction_horizon'] >= len(data):
                    continue
                    
                current_price = data.iloc[idx]['close']
                future_price = data.iloc[idx + self.config['prediction_horizon']]['close']
                
                # –í—ã—á–∏—Å–ª—è–µ–º –±—É–¥—É—â–∏–π –≤–æ–∑–≤—Ä–∞—Ç
                future_return = (future_price - current_price) / current_price
                returns.append(future_return)
                
                # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
                if future_return > self.config['take_profit_pct']:
                    label = 1  # BUY
                elif future_return < -self.config['stop_loss_pct']:
                    label = 2  # SELL
                else:
                    label = 0  # HOLD
                
                labels.append(label)
            
            # –û–±—Ä–µ–∑–∞–µ–º features_df –¥–æ —Ä–∞–∑–º–µ—Ä–∞ labels
            filtered_features_df = features_df.iloc[:len(labels)].copy()
            
            # –î–µ—Ç–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
            returns_array = np.array(returns)
            logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(labels)} –º–µ—Ç–æ–∫")
            logger.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: HOLD={labels.count(0)}, BUY={labels.count(1)}, SELL={labels.count(2)}")
            logger.info(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏:")
            logger.info(f"  –°—Ä–µ–¥–Ω—è—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: {returns_array.mean():.6f}")
            logger.info(f"  –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {returns_array.std():.6f}")
            logger.info(f"  –ú–∏–Ω–∏–º—É–º: {returns_array.min():.6f}")
            logger.info(f"  –ú–∞–∫—Å–∏–º—É–º: {returns_array.max():.6f}")
            logger.info(f"  –ü—Ä–æ—Ü–µ–Ω—Ç–∏–ª–∏: 5%={np.percentile(returns_array, 5):.6f}, 95%={np.percentile(returns_array, 95):.6f}")
            logger.info(f"–ü–æ—Ä–æ–≥–∏: take_profit={self.config['take_profit_pct']:.6f}, stop_loss={self.config['stop_loss_pct']:.6f}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–∫–æ–ª—å–∫–æ —Ç–æ—á–µ–∫ –ø—Ä–µ–≤—ã—à–∞—é—Ç –ø–æ—Ä–æ–≥–∏
            buy_candidates = np.sum(returns_array > self.config['take_profit_pct'])
            sell_candidates = np.sum(returns_array < -self.config['stop_loss_pct'])
            logger.info(f"–ö–∞–Ω–¥–∏–¥–∞—Ç—ã: BUY={buy_candidates}, SELL={sell_candidates}")
            
            return np.array(labels), filtered_features_df
    
    def train_models(self, X: pd.DataFrame, y: np.ndarray) -> Dict:
        """–û–±—É—á–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π –∫–ª–∞—Å—Å–æ–≤ –∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        logger.info("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤...")
        
        # üéØ –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø: –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–ê –ò–ù–î–ò–ö–ê–¢–û–†–û–í
        logger.info("=" * 60)
        logger.info("üîç –≠–¢–ê–ü 1: –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–ê –ò–ù–î–ò–ö–ê–¢–û–†–û–í")
        logger.info("=" * 60)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –æ—Ç–±–æ—Ä–∞ –ª—É—á—à–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        X_balanced, selected_features = self.balance_indicators(X, y, top_k_features=50)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        quality_analysis = self.analyze_feature_quality(X_balanced, y, selected_features)
        
        logger.info("=" * 60)
        logger.info("ü§ñ –≠–¢–ê–ü 2: –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô –ù–ê –û–¢–û–ë–†–ê–ù–ù–´–• –ü–†–ò–ó–ù–ê–ö–ê–•")
        logger.info("=" * 60)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        feature_cols = [col for col in selected_features if col not in ['index', 'current_price']]
        X_features = X_balanced[feature_cols]
        self.feature_names = feature_cols
        
        logger.info(f"üìä –û–±—É—á–µ–Ω–∏–µ –Ω–∞ {len(feature_cols)} –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö (–±—ã–ª–æ {len(X.columns)-2})")
        logger.info(f"üéØ –°—Ä–µ–¥–Ω—è—è —Ä–∞–∑–¥–µ–ª–∏–º–æ—Å—Ç—å –∫–ª–∞—Å—Å–æ–≤: {quality_analysis['average_separation_ratio']:.4f}")
        
        # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
        class_weights = None
        if self.config.get('use_class_weights', True):
            unique_classes = np.unique(y)
            class_weights_array = compute_class_weight('balanced', classes=unique_classes, y=y)
            class_weights = dict(zip(unique_classes, class_weights_array))
            logger.info(f"–í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: {class_weights}")
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
        X_train, X_test, y_train, y_test = train_test_split(
            X_features, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        self.scalers['main'] = scaler
        
        # –ú–æ–¥–µ–ª–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π
        models_config = {
            'random_forest': {
                'model': RandomForestClassifier(
                    random_state=42, 
                    class_weight=class_weights if class_weights else 'balanced',
                    n_jobs=-1
                ),
                'params': {
                    'n_estimators': [100, 200, 300],
                    'max_depth': [10, 20, None],
                    'min_samples_split': [2, 5, 10],
                    'min_samples_leaf': [1, 2, 4]
                }
            },
            'xgboost': {
                'model': xgb.XGBClassifier(
                    random_state=42, 
                    n_jobs=-1,
                    eval_metric='mlogloss'
                ),
                'params': {
                    'n_estimators': [100, 200],
                    'max_depth': [6, 10],
                    'learning_rate': [0.01, 0.1, 0.2],
                    'subsample': [0.8, 1.0],
                    'scale_pos_weight': [1, 2, 3] if len(np.unique(y)) == 2 else [1]
                }
            },
            'lightgbm': {
                'model': lgb.LGBMClassifier(
                    random_state=42, 
                    verbose=-1, 
                    class_weight=class_weights if class_weights else 'balanced',
                    n_jobs=-1
                ),
                'params': {
                    'n_estimators': [100, 200],
                    'max_depth': [6, 10],
                    'learning_rate': [0.01, 0.1],
                    'num_leaves': [31, 50]
                }
            },
            'gradient_boosting': {
                'model': GradientBoostingClassifier(random_state=42),
                'params': {
                    'n_estimators': [100, 200],
                    'max_depth': [6, 10],
                    'learning_rate': [0.01, 0.1]
                }
            }
        }
        
        results = {}
        
        for name, config in models_config.items():
            logger.info(f"–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏: {name}")
            
            try:
                # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–µ—Å–æ–≤ –¥–ª—è XGBoost (—Å–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞)
                if name == 'xgboost' and class_weights and len(np.unique(y)) > 2:
                    # –î–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤ XGBoost –∏—Å–ø–æ–ª—å–∑—É–µ–º sample_weight
                    sample_weights = np.array([class_weights[label] for label in y_train])
                    
                    # –û–±—É—á–µ–Ω–∏–µ –±–µ–∑ GridSearch –¥–ª—è XGBoost —Å –≤–µ—Å–∞–º–∏
                    model = config['model']
                    model.fit(X_train_scaled, y_train, sample_weight=sample_weights)
                    best_model = model
                    best_params = model.get_params()
                else:
                    # –û–±—ã—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å GridSearch
                    grid_search = GridSearchCV(
                        config['model'], 
                        config['params'], 
                        cv=3, 
                        scoring='f1_macro',
                        n_jobs=-1,
                        verbose=1
                    )
                    grid_search.fit(X_train_scaled, y_train)
                    best_model = grid_search.best_estimator_
                    best_params = grid_search.best_params_
                
                # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                y_pred = best_model.predict(X_test_scaled)
                y_pred_proba = best_model.predict_proba(X_test_scaled)
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                results[name] = {
                    'model': best_model,
                    'params': best_params,
                    'predictions': y_pred,
                    'probabilities': y_pred_proba,
                    'y_test': y_test,
                    'accuracy': accuracy_score(y_test, y_pred),
                    'classification_report': classification_report(y_test, y_pred, output_dict=True),
                    'confusion_matrix': confusion_matrix(y_test, y_pred)
                }
                
                logger.info(f"–ú–æ–¥–µ–ª—å {name} –æ–±—É—á–µ–Ω–∞. –¢–æ—á–Ω–æ—Å—Ç—å: {results[name]['accuracy']:.4f}")
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ {name}: {str(e)}")
                continue
        
        return results
    
    def evaluate_model(self, results: Dict) -> Dict:
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏"""
        logger.info("–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏...")
        
        evaluation = {}
        
        for name, result in results.items():
            y_test = result['y_test']
            y_pred = result['predictions']
            
            # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            accuracy = accuracy_score(y_test, y_pred)
            
            # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
            report = classification_report(y_test, y_pred, output_dict=True)
            
            # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
            cm = confusion_matrix(y_test, y_pred)
            
            evaluation[name] = {
                'accuracy': accuracy,
                'classification_report': report,
                'confusion_matrix': cm.tolist(),
                'meets_target': accuracy >= self.config['target_winrate']
            }
            
            logger.info(f"\n{name} - –î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞:")
            logger.info(f"Accuracy: {accuracy:.4f}")
            logger.info(f"–î–æ—Å—Ç–∏–≥–∞–µ—Ç —Ü–µ–ª–µ–≤–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ ({self.config['target_winrate']}): {accuracy >= self.config['target_winrate']}")
        
        return evaluation
    
    def save_model(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        logger.info("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        
        model_path = self.model_dir / f"{self.symbol}_trading_model.joblib"
        scaler_path = self.model_dir / f"{self.symbol}_scaler.joblib"
        config_path = self.model_dir / f"{self.symbol}_config.json"
        features_path = self.model_dir / f"{self.symbol}_features.json"
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        joblib.dump(self.models['best'], model_path)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫–µ–π–ª–µ—Ä
        joblib.dump(self.scalers['main'], scaler_path)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        with open(config_path, 'w') as f:
            json.dump(self.config, f, indent=2)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        with open(features_path, 'w') as f:
            json.dump(self.feature_names, f, indent=2)
        
        logger.info(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_path}")
    
    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
        
        model_path = self.model_dir / f"{self.symbol}_trading_model.joblib"
        scaler_path = self.model_dir / f"{self.symbol}_scaler.joblib"
        config_path = self.model_dir / f"{self.symbol}_config.json"
        features_path = self.model_dir / f"{self.symbol}_features.json"
        
        if not all(p.exists() for p in [model_path, scaler_path, config_path, features_path]):
            raise FileNotFoundError("–ù–µ –Ω–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        self.models['best'] = joblib.load(model_path)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–∫–µ–π–ª–µ—Ä
        self.scalers['main'] = joblib.load(scaler_path)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        with open(config_path, 'r') as f:
            self.config = json.load(f)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        with open(features_path, 'r') as f:
            self.feature_names = json.load(f)
        
        logger.info("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
    
    async def backtest(self, data: pd.DataFrame) -> Dict:
        """–ë—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""
        logger.info("–ó–∞–ø—É—Å–∫ –±—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥–∞...")
        
        if 'best' not in self.models:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ.")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –±—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥–∞
        features_df = self.prepare_features(data)
        
        # –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        X = features_df[self.feature_names]
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        X_scaled = self.scalers['main'].transform(X)
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        predictions = self.models['best'].predict(X_scaled)
        probabilities = self.models['best'].predict_proba(X_scaled)
        
        # –°–∏–º—É–ª—è—Ü–∏—è —Ç–æ—Ä–≥–æ–≤–ª–∏
        initial_balance = 10000
        balance = initial_balance
        position = 0
        trades = []
        
        for i, (pred, prob) in enumerate(zip(predictions, probabilities)):
            if i + self.config['lookback_period'] >= len(data):
                break
                
            current_price = data.iloc[i + self.config['lookback_period']]['close']
            confidence = np.max(prob)
            
            # –¢–æ—Ä–≥–æ–≤–∞—è –ª–æ–≥–∏–∫–∞
            if pred == 1 and confidence > self.config['min_confidence'] and position <= 0:  # BUY
                if position < 0:  # –ó–∞–∫—Ä—ã–≤–∞–µ–º –∫–æ—Ä–æ—Ç–∫—É—é –ø–æ–∑–∏—Ü–∏—é
                    balance += position * current_price
                    trades.append({
                        'type': 'close_short',
                        'price': current_price,
                        'balance': balance,
                        'confidence': confidence
                    })
                
                # –û—Ç–∫—Ä—ã–≤–∞–µ–º –¥–ª–∏–Ω–Ω—É—é –ø–æ–∑–∏—Ü–∏—é
                position = balance / current_price
                balance = 0
                trades.append({
                    'type': 'buy',
                    'price': current_price,
                    'position': position,
                    'confidence': confidence
                })
                
            elif pred == 2 and confidence > self.config['min_confidence'] and position >= 0:  # SELL
                if position > 0:  # –ó–∞–∫—Ä—ã–≤–∞–µ–º –¥–ª–∏–Ω–Ω—É—é –ø–æ–∑–∏—Ü–∏—é
                    balance += position * current_price
                    trades.append({
                        'type': 'close_long',
                        'price': current_price,
                        'balance': balance,
                        'confidence': confidence
                    })
                
                # –û—Ç–∫—Ä—ã–≤–∞–µ–º –∫–æ—Ä–æ—Ç–∫—É—é –ø–æ–∑–∏—Ü–∏—é
                position = -balance / current_price
                balance = 0
                trades.append({
                    'type': 'sell',
                    'price': current_price,
                    'position': position,
                    'confidence': confidence
                })
        
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º –æ—Å—Ç–∞–≤—à—É—é—Å—è –ø–æ–∑–∏—Ü–∏—é
        final_price = data.iloc[-1]['close']
        if position != 0:
            balance += position * final_price
        
        # –†–∞—Å—á–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        total_return = (balance - initial_balance) / initial_balance * 100
        num_trades = len(trades)
        
        # –ü–æ–¥—Å—á–µ—Ç –≤—ã–∏–≥—Ä—ã—à–Ω—ã—Ö —Å–¥–µ–ª–æ–∫
        winning_trades = 0
        for i in range(1, len(trades)):
            if trades[i]['type'] in ['close_long', 'close_short']:
                if trades[i]['balance'] > trades[i-1].get('balance', initial_balance):
                    winning_trades += 1
        
        win_rate = winning_trades / (num_trades // 2) if num_trades > 0 else 0
        
        results = {
            'initial_balance': initial_balance,
            'final_balance': balance,
            'total_return_pct': total_return,
            'num_trades': num_trades,
            'win_rate': win_rate,
            'trades': trades,
            'meets_target_winrate': win_rate >= self.config['target_winrate']
        }
        
        logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥–∞:")
        logger.info(f"–û–±—â–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: {total_return:.2f}%")
        logger.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–¥–µ–ª–æ–∫: {num_trades}")
        logger.info(f"–í–∏–Ω—Ä–µ–π—Ç: {win_rate:.2f}")
        logger.info(f"–î–æ—Å—Ç–∏–≥–∞–µ—Ç —Ü–µ–ª–µ–≤–æ–π –≤–∏–Ω—Ä–µ–π—Ç ({self.config['target_winrate']}): {win_rate >= self.config['target_winrate']}")
        
        return results

async def main():
    parser = argparse.ArgumentParser(description='Trading AI Trainer')
    parser.add_argument('--symbol', default='BTCUSDT', help='–¢–æ—Ä–≥–æ–≤–∞—è –ø–∞—Ä–∞')
    parser.add_argument('--days', type=int, default=365, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö')
    parser.add_argument('--train', action='store_true', help='–ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ')
    parser.add_argument('--backtest', action='store_true', help='–ó–∞–ø—É—Å—Ç–∏—Ç—å –±—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥')
    parser.add_argument('--evaluate', action='store_true', help='–û—Ü–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å')
    
    args = parser.parse_args()
    
    trainer = TradingAITrainer(args.symbol)
    
    if args.train:
        logger.info("=== –†–ï–ñ–ò–ú –û–ë–£–ß–ï–ù–ò–Ø ===")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        data = await trainer.load_market_data(args.days)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        features_df = trainer.prepare_features(data)
        
        # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫–∏
        labels, features_df = trainer.create_labels(data, features_df)
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏
        results = trainer.train_models(features_df, labels)
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –º–æ–¥–µ–ª–∏
        evaluation = trainer.evaluate_model(results)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        trainer.save_model()
        
        logger.info("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    
    if args.backtest:
        logger.info("=== –†–ï–ñ–ò–ú –ë–≠–ö–¢–ï–°–¢–ò–ù–ì–ê ===")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        trainer.load_model()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        data = await trainer.load_market_data(args.days)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –±—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥
        backtest_results = await trainer.backtest(data)
        
        logger.info("–ë—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    
    if args.evaluate:
        logger.info("=== –†–ï–ñ–ò–ú –û–¶–ï–ù–ö–ò ===")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        trainer.load_model()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        data = await trainer.load_market_data(args.days)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        features_df = trainer.prepare_features(data)
        labels, features_df = trainer.create_labels(data, features_df)
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        X = features_df[trainer.feature_names]
        X_scaled = trainer.scalers['main'].transform(X)
        predictions = trainer.models['best'].predict(X_scaled)
        
        from sklearn.metrics import accuracy_score
        accuracy = accuracy_score(labels, predictions)
        
        logger.info(f"–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {accuracy:.4f}")

if __name__ == "__main__":
    asyncio.run(main())