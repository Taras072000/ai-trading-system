"""
LGBM AI –º–æ–¥—É–ª—å –¥–ª—è Peper Binance v4
–õ–µ–≥–∫–æ–≤–µ—Å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LightGBM
"""

import asyncio
import logging
import numpy as np
import pandas as pd
import lightgbm as lgb
from typing import Dict, List, Optional, Any, Tuple, Union
from datetime import datetime, timedelta
import gc
import pickle
import os
from dataclasses import dataclass
import config
from utils.timezone_utils import get_utc_now
from config_params import CONFIG_PARAMS
from utils.indicators_cache import indicators_cache
from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, mean_squared_error

logger = logging.getLogger(__name__)

@dataclass
class LGBMPrediction:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è LGBM –º–æ–¥–µ–ª–∏"""
    prediction: Union[float, List[float]]
    confidence: float
    feature_importance: Dict[str, float]
    model_type: str
    timestamp: datetime
    metadata: Dict[str, Any]

class LGBMModelManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –º–æ–¥–µ–ª–µ–π LGBM —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞–º—è—Ç–∏"""
    
    def __init__(self, max_models: int = 3):
        self.max_models = max_models
        self.models = {}
        self.scalers = {}
        self.model_metadata = {}
        self.last_used = {}
    
    def add_model(self, name: str, model: lgb.LGBMModel, scaler: StandardScaler = None):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –ø–∞–º—è—Ç–∏"""
        if len(self.models) >= self.max_models:
            self._remove_oldest_model()
        
        self.models[name] = model
        self.scalers[name] = scaler
        self.last_used[name] = get_utc_now()
        self.model_metadata[name] = {
            'created': get_utc_now(),
            'size_mb': self._estimate_model_size(model)
        }
    
    def get_model(self, name: str) -> Tuple[Optional[lgb.LGBMModel], Optional[StandardScaler]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        if name in self.models:
            self.last_used[name] = get_utc_now()
            return self.models[name], self.scalers.get(name)
        return None, None
    
    def _remove_oldest_model(self):
        """–£–¥–∞–ª–µ–Ω–∏–µ —Å–∞–º–æ–π —Å—Ç–∞—Ä–æ–π –º–æ–¥–µ–ª–∏"""
        if not self.last_used:
            return
        
        oldest_model = min(self.last_used.items(), key=lambda x: x[1])[0]
        self.remove_model(oldest_model)
    
    def remove_model(self, name: str):
        """–£–¥–∞–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        self.models.pop(name, None)
        self.scalers.pop(name, None)
        self.last_used.pop(name, None)
        self.model_metadata.pop(name, None)
        gc.collect()
    
    def _estimate_model_size(self, model: lgb.LGBMModel) -> float:
        """–ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ –≤ –ú–ë"""
        try:
            # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–µ—Ä–µ–≤—å–µ–≤ –∏ –ª–∏—Å—Ç—å–µ–≤
            if hasattr(model, 'booster_'):
                return model.booster_.num_trees() * 0.1  # –ü—Ä–∏–º–µ—Ä–Ω–æ 0.1 –ú–ë –Ω–∞ –¥–µ—Ä–µ–≤–æ
            return 1.0  # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
        except:
            return 1.0

class LGBMAI:
    """
    LGBM AI –º–æ–¥—É–ª—å —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π —Ä–µ—Å—É—Ä—Å–æ–≤
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –∑–∞–¥–∞—á –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    """
    
    def __init__(self):
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é LGBM AI –∏–∑ CONFIG_PARAMS
        ai_config = CONFIG_PARAMS.get('ai_modules', {})
        lgbm_config = ai_config.get('lgbm', {})
        
        self.config = lgbm_config
        self.is_initialized = False
        self.model_manager = LGBMModelManager()
        self.feature_cache = {}
        self.last_cleanup = get_utc_now()
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã LGBM –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
        self.lgbm_params = {
            'objective': 'regression',
            'metric': 'rmse',
            'boosting_type': 'gbdt',
            'num_leaves': lgbm_config.get('num_leaves', 15),  # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å 31 –¥–æ 15
            'max_depth': lgbm_config.get('max_depth', 4),     # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å 6 –¥–æ 4
            'learning_rate': lgbm_config.get('learning_rate', 0.05),  # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å 0.1 –¥–æ 0.05
            'n_estimators': lgbm_config.get('n_estimators', 100),
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'random_state': 42,
            'n_jobs': 1,  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤
            'verbose': -1,
            'force_row_wise': True  # –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏
        }
        
        logger.info("LGBM AI –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π —Ä–µ—Å—É—Ä—Å–æ–≤")
    
    async def initialize(self):
        """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥—É–ª—è"""
        if self.is_initialized:
            return True
        
        try:
            logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LGBM AI –º–æ–¥—É–ª—è...")
            
            # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏
            await self._create_base_models()
            
            self.is_initialized = True
            logger.info("LGBM AI –º–æ–¥—É–ª—å —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ LGBM AI: {e}")
            return False
    
    async def _create_base_models(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—ã–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        np.random.seed(42)
        X_demo = np.random.randn(100, 5)
        y_demo = np.random.randn(100)
        
        # –ú–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ü–µ–Ω—ã
        price_model = lgb.LGBMRegressor(**self.lgbm_params)
        price_model.fit(X_demo, y_demo)
        
        # –ú–æ–¥–µ–ª—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç—Ä–µ–Ω–¥–∞
        trend_params = self.lgbm_params.copy()
        trend_params['objective'] = 'binary'
        trend_params['metric'] = 'binary_logloss'
        
        y_trend = (y_demo > 0).astype(int)
        trend_model = lgb.LGBMClassifier(**trend_params)
        trend_model.fit(X_demo, y_trend)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–æ–¥–µ–ª–∏ –≤ –º–µ–Ω–µ–¥–∂–µ—Ä
        self.model_manager.add_model('price_prediction', price_model)
        self.model_manager.add_model('trend_classification', trend_model)
        
        logger.info("–ë–∞–∑–æ–≤—ã–µ LGBM –º–æ–¥–µ–ª–∏ —Å–æ–∑–¥–∞–Ω—ã")
    
    async def train_model(self, model_name: str, X: pd.DataFrame, y: pd.Series, 
                         model_type: str = 'regression') -> Dict[str, Any]:
        """
        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π TimeSeriesSplit
        """
        if not self.is_initialized:
            await self.initialize()
        
        try:
            logger.info(f"–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ {model_name} —Å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π...")
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            X_processed, scaler = await self._prepare_features(X)
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏
            params = self.lgbm_params.copy()
            if model_type == 'classification':
                params['objective'] = 'binary'
                params['metric'] = 'binary_logloss'
                model = lgb.LGBMClassifier(**params)
                scoring = 'accuracy'
            else:
                model = lgb.LGBMRegressor(**params)
                scoring = 'neg_mean_squared_error'
            
            # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è —Å TimeSeriesSplit (5 —Ñ–æ–ª–¥–æ–≤)
            tscv = TimeSeriesSplit(n_splits=5)
            cv_scores = cross_val_score(model, X_processed, y, cv=tscv, scoring=scoring, n_jobs=1)
            
            # –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
            X_train, X_test, y_train, y_test = train_test_split(
                X_processed, y, test_size=0.2, random_state=42, shuffle=False  # –ë–µ–∑ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏—è –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
            )
            
            # –û–±—É—á–µ–Ω–∏–µ —Å —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–æ–π
            model.fit(
                X_train, y_train,
                eval_set=[(X_test, y_test)],
                callbacks=[lgb.early_stopping(10), lgb.log_evaluation(0)]
            )
            
            # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            if model_type == 'classification':
                y_pred = model.predict(X_test)
                test_score = accuracy_score(y_test, y_pred)
                metric_name = 'accuracy'
                cv_mean = cv_scores.mean()
                cv_std = cv_scores.std()
            else:
                y_pred = model.predict(X_test)
                test_score = mean_squared_error(y_test, y_pred, squared=False)
                metric_name = 'rmse'
                # –î–ª—è MSE –Ω—É–∂–Ω–æ –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                cv_mean = np.sqrt(-cv_scores.mean())
                cv_std = np.sqrt(cv_scores.std())
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            self.model_manager.add_model(model_name, model, scaler)
            
            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
            await self._periodic_cleanup()
            
            logger.info(f"–ú–æ–¥–µ–ª—å {model_name} –æ–±—É—á–µ–Ω–∞. CV Score: {cv_mean:.4f} ¬± {cv_std:.4f}, Test Score: {test_score:.4f}")
            
            return {
                'model_name': model_name,
                'model_type': model_type,
                'test_score': test_score,
                'cv_score_mean': cv_mean,
                'cv_score_std': cv_std,
                'cv_scores': cv_scores.tolist(),
                'metric': metric_name,
                'feature_importance': dict(zip(X.columns, model.feature_importances_)),
                'training_samples': len(X_train),
                'test_samples': len(X_test),
                'cross_validation': {
                    'method': 'TimeSeriesSplit',
                    'n_splits': 5,
                    'scores': cv_scores.tolist()
                }
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            return {'error': str(e)}
    
    async def predict(self, model_name: str, X: pd.DataFrame) -> LGBMPrediction:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        """
        if not self.is_initialized:
            await self.initialize()
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            model, scaler = self.model_manager.get_model(model_name)
            if model is None:
                raise ValueError(f"–ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            X_processed = X.copy()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            if hasattr(model, 'n_features_in_'):
                expected_features = model.n_features_in_
                if X_processed.shape[1] != expected_features:
                    logger.warning(f"–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: –æ–∂–∏–¥–∞–µ—Ç—Å—è {expected_features}, –ø–æ–ª—É—á–µ–Ω–æ {X_processed.shape[1]}")
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ n –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–ª–∏ –¥–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
                    if X_processed.shape[1] > expected_features:
                        X_processed = X_processed.iloc[:, :expected_features]
                    else:
                        # –î–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏ –¥–æ –Ω—É–∂–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
                        missing_cols = expected_features - X_processed.shape[1]
                        for i in range(missing_cols):
                            X_processed[f'feature_{X_processed.shape[1] + i}'] = 0.0
            
            if scaler is not None:
                X_processed = pd.DataFrame(
                    scaler.transform(X_processed),
                    columns=X_processed.columns,
                    index=X_processed.index
                )
            
            # –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            prediction = model.predict(X_processed)
            
            # –í—ã—á–∏—Å–ª—è–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (–¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)
            if hasattr(model, 'predict_proba'):
                proba = model.predict_proba(X_processed)
                confidence = np.max(proba, axis=1).mean()
            else:
                # –î–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—Ä–∞—Ç–Ω—É—é –≤–µ–ª–∏—á–∏–Ω—É —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è
                confidence = 1.0 / (1.0 + np.std(prediction))
            
            # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            feature_importance = {}
            if hasattr(model, 'feature_importances_'):
                feature_importance = dict(zip(X.columns, model.feature_importances_))
            
            return LGBMPrediction(
                prediction=prediction.tolist() if len(prediction) > 1 else float(prediction[0]),
                confidence=float(confidence),
                feature_importance=feature_importance,
                model_type=type(model).__name__,
                timestamp=get_utc_now(),
                metadata={
                    'model_name': model_name,
                    'samples_predicted': len(X),
                    'features_used': list(X.columns)
                }
            )
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            return LGBMPrediction(
                prediction=0.0,
                confidence=0.0,
                feature_importance={},
                model_type='error',
                timestamp=get_utc_now(),
                metadata={'error': str(e)}
            )
    
    async def _prepare_features(self, X: pd.DataFrame) -> Tuple[pd.DataFrame, StandardScaler]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        # –ü—Ä–æ—Å—Ç–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X_processed = X.copy()
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        X_processed = X_processed.fillna(X_processed.mean())
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        scaler = StandardScaler()
        X_scaled = pd.DataFrame(
            scaler.fit_transform(X_processed),
            columns=X_processed.columns,
            index=X_processed.index
        )
        
        return X_scaled, scaler
    
    async def create_trading_features(self, price_data: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ —Ü–µ–Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (—Ä–æ–≤–Ω–æ 5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
        try:
            features = pd.DataFrame(index=price_data.index)
            
            # –°–æ–∑–¥–∞–µ–º —Ä–æ–≤–Ω–æ 5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é
            # 1. –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Ü–µ–Ω–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à –¥–ª—è SMA)
            sma_20 = indicators_cache.get_sma(price_data['close'], 20)
            if sma_20 is not None:
                sma_20_std = price_data['close'].rolling(20).std()
                features['price_norm'] = (price_data['close'] - sma_20) / sma_20_std
            else:
                features['price_norm'] = 0.0
            
            # 2. RSI (–∏—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à)
            rsi = indicators_cache.get_rsi(price_data['close'], 14)
            if rsi is not None:
                features['rsi'] = rsi / 100.0  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ [0,1]
            else:
                features['rsi'] = 0.5  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            
            # 3. –û—Ç–Ω–æ—à–µ–Ω–∏–µ SMA (–∏—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à)
            sma_5 = indicators_cache.get_sma(price_data['close'], 5)
            if sma_5 is not None and sma_20 is not None:
                features['sma_ratio'] = sma_5 / sma_20 - 1.0  # –¶–µ–Ω—Ç—Ä–∏—Ä—É–µ–º –≤–æ–∫—Ä—É–≥ 0
            else:
                features['sma_ratio'] = 0.0
            
            # 4. –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å (–∏—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à)
            volatility = indicators_cache.get_volatility(price_data['close'], 10)
            if volatility is not None:
                features['volatility'] = volatility
            else:
                features['volatility'] = 0.0
            
            # 5. –¶–µ–Ω–æ–≤–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ
            features['price_change'] = price_data['close'].pct_change()
            
            # –£–¥–∞–ª—è–µ–º NaN –∏ –∑–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
            features = features.fillna(0.0)
            
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —É –Ω–∞—Å —Ä–æ–≤–Ω–æ 5 –∫–æ–ª–æ–Ω–æ–∫
            if features.shape[1] != 5:
                logger.warning(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {features.shape[1]}, –æ–∂–∏–¥–∞–µ—Ç—Å—è 5")
                # –û–±—Ä–µ–∑–∞–µ–º –¥–æ 5 –∏–ª–∏ –¥–æ–ø–æ–ª–Ω—è–µ–º
                if features.shape[1] > 5:
                    features = features.iloc[:, :5]
                else:
                    for i in range(5 - features.shape[1]):
                        features[f'dummy_{i}'] = 0.0
            
            return features
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
            return pd.DataFrame()
    
    async def predict_price_movement(self, price_data: pd.DataFrame) -> LGBMPrediction:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã"""
        try:
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            features = await self.create_trading_features(price_data)
            if features.empty:
                raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            latest_features = features.tail(1)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏ —Ç—Ä–µ–Ω–¥–∞
            prediction = await self.predict('trend_classification', latest_features)
            
            return prediction
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã: {e}")
            return LGBMPrediction(
                prediction=0.5,  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                confidence=0.0,
                feature_importance={},
                model_type='error',
                timestamp=get_utc_now(),
                metadata={'error': str(e)}
            )
    
    async def predict_market_direction(self, symbol: str, price_data: pd.DataFrame) -> Dict[str, Any]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä—ã–Ω–∫–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Ç–µ—Å—Ç–∞–º–∏"""
        try:
            if not self.is_initialized:
                await self.initialize()
            
            # –°–æ–∑–¥–∞–µ–º —Ç–æ—Ä–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            features = await self.create_trading_features(price_data)
            if features.empty:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è {symbol}")
                return {
                    'direction': 0.0,
                    'confidence': 0.1,
                    'reasoning': '–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏',
                    'model_type': 'lgbm_ai'
                }
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            latest_features = features.tail(1)
            
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ—Ç –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç—Ä–µ–Ω–¥–∞
            prediction = await self.predict('trend_classification', latest_features)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç, –æ–∂–∏–¥–∞–µ–º—ã–π —Ç–µ—Å—Ç–∞–º–∏
            if prediction.model_type == 'error':
                direction = 0.0
                confidence = 0.1
                reasoning = f"–û—à–∏–±–∫–∞ –º–æ–¥–µ–ª–∏: {prediction.metadata.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}"
            else:
                # –î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ prediction.prediction - —ç—Ç–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∞ 1 (—Ä–æ—Å—Ç)
                raw_prediction = prediction.prediction
                if isinstance(raw_prediction, list):
                    raw_prediction = raw_prediction[0] if raw_prediction else 0.5
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: >0.5 = —Ä–æ—Å—Ç, <0.5 = –ø–∞–¥–µ–Ω–∏–µ
                direction = 1.0 if raw_prediction > 0.5 else -1.0
                confidence = prediction.confidence
                
                # –°–æ–∑–¥–∞–µ–º –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ
                trend_word = "—Ä–æ—Å—Ç" if direction > 0 else "–ø–∞–¥–µ–Ω–∏–µ"
                reasoning = f"LGBM AI: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ {trend_word} —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é {raw_prediction:.3f}"
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                if prediction.feature_importance:
                    top_features = sorted(prediction.feature_importance.items(), 
                                        key=lambda x: x[1], reverse=True)[:2]
                    feature_info = ", ".join([f"{feat}: {imp:.3f}" for feat, imp in top_features])
                    reasoning += f" (–∫–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {feature_info})"
            
            logger.info(f"ü§ñ LGBM AI —Å–∏–≥–Ω–∞–ª –¥–ª—è {symbol}: –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ={direction}, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å={confidence*100:.1f}%")
            
            return {
                'direction': direction,
                'confidence': confidence,
                'reasoning': reasoning,
                'model_type': 'lgbm_ai',
                'symbol': symbol,
                'prediction_raw': prediction.prediction,
                'feature_importance': prediction.feature_importance,
                'timestamp': get_utc_now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä—ã–Ω–∫–∞ –¥–ª—è {symbol}: {e}")
            return {
                'direction': 0.0,
                'confidence': 0.1,
                'reasoning': f'–û—à–∏–±–∫–∞ LGBM AI: {str(e)}',
                'model_type': 'lgbm_ai',
                'symbol': symbol,
                'error': str(e)
            }
    
    async def _periodic_cleanup(self):
        """–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏"""
        now = get_utc_now()
        if (now - self.last_cleanup).seconds > 300:  # –ö–∞–∂–¥—ã–µ 5 –º–∏–Ω—É—Ç
            # –û—á–∏—â–∞–µ–º –∫—ç—à –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            if len(self.feature_cache) > 10:
                # –£–¥–∞–ª—è–µ–º –ø–æ–ª–æ–≤–∏–Ω—É —Å–∞–º—ã—Ö —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π
                sorted_items = sorted(self.feature_cache.items())
                for key in sorted_items[:len(sorted_items)//2]:
                    del self.feature_cache[key[0]]
            
            gc.collect()
            self.last_cleanup = now
            logger.debug("–í—ã–ø–æ–ª–Ω–µ–Ω–∞ –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ LGBM AI")
    
    async def save_model(self, model_name: str, filepath: str) -> bool:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∏—Å–∫"""
        try:
            model, scaler = self.model_manager.get_model(model_name)
            if model is None:
                return False
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∏ —Å–∫–µ–π–ª–µ—Ä
            model_data = {
                'model': model,
                'scaler': scaler,
                'metadata': self.model_manager.model_metadata.get(model_name, {}),
                'timestamp': get_utc_now()
            }
            
            with open(filepath, 'wb') as f:
                pickle.dump(model_data, f)
            
            logger.info(f"–ú–æ–¥–µ–ª—å {model_name} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {filepath}")
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            return False
    
    async def load_model(self, model_name: str, filepath: str) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –¥–∏—Å–∫–∞"""
        try:
            if not os.path.exists(filepath):
                return False
            
            with open(filepath, 'rb') as f:
                model_data = pickle.load(f)
            
            self.model_manager.add_model(
                model_name,
                model_data['model'],
                model_data.get('scaler')
            )
            
            logger.info(f"–ú–æ–¥–µ–ª—å {model_name} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {filepath}")
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            return False
    
    async def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ –º–æ–¥—É–ª—è"""
        logger.info("–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ LGBM AI...")
        
        # –û—á–∏—â–∞–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏
        for model_name in list(self.model_manager.models.keys()):
            self.model_manager.remove_model(model_name)
        
        self.feature_cache.clear()
        
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞
        gc.collect()
        
        self.is_initialized = False
        logger.info("LGBM AI —Ä–µ—Å—É—Ä—Å—ã –æ—á–∏—â–µ–Ω—ã")
    
    def get_memory_usage(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –ø–∞–º—è—Ç–∏"""
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()
        
        total_model_size = sum(
            metadata.get('size_mb', 0) 
            for metadata in self.model_manager.model_metadata.values()
        )
        
        return {
            'rss_mb': memory_info.rss / 1024 / 1024,
            'models_loaded': len(self.model_manager.models),
            'total_model_size_mb': total_model_size,
            'feature_cache_size': len(self.feature_cache),
            'model_names': list(self.model_manager.models.keys())
        }
    
    async def generate_trading_signals(self, symbol: str, data: pd.DataFrame) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Å–∏—Å—Ç–µ–º–æ–π —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        try:
            if not self.is_initialized:
                await self.initialize()
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –º–µ—Ç–æ–¥ predict_market_direction
            market_prediction = await self.predict_market_direction(symbol, data)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç —Ç–æ—Ä–≥–æ–≤–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞
            direction = market_prediction.get('direction', 0.0)
            confidence = market_prediction.get('confidence', 0.1)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å–∏–≥–Ω–∞–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            if abs(direction) < 0.1 or confidence < 0.3:
                signal_type = 'no_signal'
                action = 'HOLD'
            elif direction > 0:
                signal_type = 'buy_signal'
                action = 'BUY'
            else:
                signal_type = 'sell_signal'
                action = 'SELL'
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º take profit –∏ stop loss –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
            if len(data) >= 20:
                volatility = data['close'].pct_change().rolling(20).std().iloc[-1]
                if pd.isna(volatility):
                    volatility = 0.02  # –î–µ—Ñ–æ–ª—Ç–Ω–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å 2%
            else:
                volatility = 0.02
            
            take_profit = 2.0 * volatility * 100  # 2x –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
            stop_loss = 1.5 * volatility * 100    # 1.5x –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
            
            current_price = data['close'].iloc[-1] if len(data) > 0 else 0
            
            signal = {
                'signal_type': signal_type,
                'action': action,
                'confidence': confidence,
                'symbol': symbol,
                'price': current_price,
                'take_profit_pct': take_profit,
                'stop_loss_pct': stop_loss,
                'reasoning': market_prediction.get('reasoning', 'LGBM AI –∞–Ω–∞–ª–∏–∑'),
                'model_name': 'lgbm_ai',
                'timestamp': get_utc_now().isoformat(),
                'direction': direction,
                'feature_importance': market_prediction.get('feature_importance', {}),
                'metadata': {
                    'volatility': volatility,
                    'data_points': len(data),
                    'prediction_raw': market_prediction.get('prediction_raw', 0.5)
                }
            }
            
            logger.info(f"ü§ñ LGBM AI —Å–∏–≥–Ω–∞–ª –¥–ª—è {symbol}: {action} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence*100:.1f}%)")
            
            return signal
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–æ—Ä–≥–æ–≤–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞ LGBM AI –¥–ª—è {symbol}: {e}")
            return {
                'signal_type': 'no_signal',
                'action': 'HOLD',
                'confidence': 0.0,
                'symbol': symbol,
                'price': 0,
                'reasoning': f'–û—à–∏–±–∫–∞ LGBM AI: {str(e)}',
                'model_name': 'lgbm_ai',
                'error': str(e),
                'timestamp': get_utc_now().isoformat()
            }
    
    async def analyze_market_data(self, symbol: str, data: pd.DataFrame) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ä—ã–Ω–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Å–∏—Å—Ç–µ–º–æ–π —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        try:
            if not self.is_initialized:
                await self.initialize()
            
            if len(data) < 20:
                return {
                    'analysis_type': 'insufficient_data',
                    'symbol': symbol,
                    'confidence': 0.0,
                    'reasoning': '–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–º–∏–Ω–∏–º—É–º 20 —Å–≤–µ—á–µ–π)',
                    'model_name': 'lgbm_ai',
                    'timestamp': get_utc_now().isoformat()
                }
            
            # –°–æ–∑–¥–∞–µ–º —Ç–æ—Ä–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            features = await self.create_trading_features(data)
            if features.empty:
                return {
                    'analysis_type': 'feature_error',
                    'symbol': symbol,
                    'confidence': 0.0,
                    'reasoning': '–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ç–æ—Ä–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏',
                    'model_name': 'lgbm_ai',
                    'timestamp': get_utc_now().isoformat()
                }
            
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã
            prediction = await self.predict_market_direction(symbol, data)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä—ã–Ω–æ—á–Ω—ã–µ —É—Å–ª–æ–≤–∏—è
            current_price = data['close'].iloc[-1]
            price_change_24h = ((current_price - data['close'].iloc[-24]) / data['close'].iloc[-24] * 100) if len(data) >= 24 else 0
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
            volatility = data['close'].pct_change().rolling(20).std().iloc[-1] * 100
            if pd.isna(volatility):
                volatility = 2.0
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä—ã–Ω–æ—á–Ω—ã–µ —É—Å–ª–æ–≤–∏—è
            if volatility > 5.0:
                market_condition = 'high_volatility'
            elif volatility < 1.0:
                market_condition = 'low_volatility'
            else:
                market_condition = 'normal_volatility'
            
            # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–∞
            sma_20 = data['close'].rolling(20).mean().iloc[-1]
            if current_price > sma_20 * 1.02:
                trend = 'uptrend'
            elif current_price < sma_20 * 0.98:
                trend = 'downtrend'
            else:
                trend = 'sideways'
            
            analysis = {
                'analysis_type': 'market_analysis',
                'symbol': symbol,
                'confidence': prediction.get('confidence', 0.5),
                'reasoning': f"LGBM AI –∞–Ω–∞–ª–∏–∑: {trend}, {market_condition}",
                'model_name': 'lgbm_ai',
                'timestamp': get_utc_now().isoformat(),
                'market_data': {
                    'current_price': current_price,
                    'price_change_24h': price_change_24h,
                    'volatility': volatility,
                    'trend': trend,
                    'market_condition': market_condition,
                    'sma_20': sma_20
                },
                'prediction': {
                    'direction': prediction.get('direction', 0.0),
                    'confidence': prediction.get('confidence', 0.5),
                    'feature_importance': prediction.get('feature_importance', {})
                },
                'technical_indicators': {
                    'rsi': features['rsi'].iloc[-1] * 100 if not features.empty else 50,
                    'sma_ratio': features['sma_ratio'].iloc[-1] if not features.empty else 0,
                    'volatility_norm': features['volatility'].iloc[-1] if not features.empty else 0
                }
            }
            
            logger.info(f"üìä LGBM AI –∞–Ω–∞–ª–∏–∑ –¥–ª—è {symbol}: {trend}, –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å {volatility:.2f}%")
            
            return analysis
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ä—ã–Ω–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö LGBM AI –¥–ª—è {symbol}: {e}")
            return {
                'analysis_type': 'error',
                'symbol': symbol,
                'confidence': 0.0,
                'reasoning': f'–û—à–∏–±–∫–∞ LGBM AI: {str(e)}',
                'model_name': 'lgbm_ai',
                'error': str(e),
                'timestamp': get_utc_now().isoformat()
            }