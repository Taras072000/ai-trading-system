#!/usr/bin/env python3
"""
–°–∏—Å—Ç–µ–º–∞ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —Ç–æ—Ä–≥–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã Peper Binance v4
–û—Å–Ω–æ–≤–Ω–æ–π –¥–≤–∏–∂–æ–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è AI –º–æ–¥–µ–ª–µ–π
"""

import asyncio
import logging
import json
import uuid
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
import numpy as np
import pandas as pd

logger = logging.getLogger(__name__)

@dataclass
class ReinforcementConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º"""
    learning_rate: float = 0.01
    reward_multiplier: float = 1.5
    punishment_multiplier: float = 0.8
    weight_decay: float = 0.001
    min_weight: float = 0.05
    max_weight: float = 0.70
    max_iterations: int = 10
    target_win_rate: float = 0.65
    min_trades_per_symbol: int = 5

@dataclass
class WeightChange:
    """–ó–∞–ø–∏—Å—å –æ–± –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏"""
    change_id: str
    session_id: str
    model_id: str
    old_weight: float
    new_weight: float
    timestamp: datetime
    reason: str
    trade_context: Dict[str, Any]

@dataclass
class ReinforcementSession:
    """–°–µ—Å—Å–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º"""
    session_id: str
    start_time: datetime
    end_time: Optional[datetime]
    config: ReinforcementConfig
    status: str  # 'running', 'completed', 'failed', 'stopped'
    initial_weights: Dict[str, float]
    final_weights: Dict[str, float]
    performance_metrics: Dict[str, Any]

class ReinforcementLearningEngine:
    """
    –û—Å–Ω–æ–≤–Ω–æ–π –¥–≤–∏–∂–æ–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º
    –£–ø—Ä–∞–≤–ª—è–µ—Ç –≤–µ—Å–∞–º–∏ AI –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–æ—Ä–≥–æ–≤–ª–∏
    """
    
    def __init__(self, config: ReinforcementConfig = None):
        self.config = config or ReinforcementConfig()
        
        # –ù–∞—á–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ AI –º–æ–¥–µ–ª–µ–π (—Å—É–º–º–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å 1.0)
        self.model_weights = {
            'trading_ai': 0.25,
            'lava_ai': 0.35, 
            'lgbm_ai': 0.40,
            'mistral_ai': 0.0  # –ù–∞—á–∏–Ω–∞–µ–º —Å 0, –±—É–¥–µ—Ç —É–≤–µ–ª–∏—á–∏–≤–∞—Ç—å—Å—è –ø—Ä–∏ —Ö–æ—Ä–æ—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
        }
        
        # –ò—Å—Ç–æ—Ä–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤–µ—Å–æ–≤
        self.weight_history: List[WeightChange] = []
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
        self.model_performance = {
            'trading_ai': {'wins': 0, 'losses': 0, 'total_pnl': 0.0, 'trades': []},
            'lava_ai': {'wins': 0, 'losses': 0, 'total_pnl': 0.0, 'trades': []},
            'lgbm_ai': {'wins': 0, 'losses': 0, 'total_pnl': 0.0, 'trades': []},
            'mistral_ai': {'wins': 0, 'losses': 0, 'total_pnl': 0.0, 'trades': []}
        }
        
        # –¢–µ–∫—É—â–∞—è —Å–µ—Å—Å–∏—è –æ–±—É—á–µ–Ω–∏—è
        self.current_session: Optional[ReinforcementSession] = None
        
        logger.info("üß† ReinforcementLearningEngine –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        logger.info(f"üìä –ù–∞—á–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ –º–æ–¥–µ–ª–µ–π: {self.model_weights}")
    
    def start_session(self, session_id: str = None) -> str:
        """–ù–∞—á–∞—Ç—å –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º"""
        if session_id is None:
            session_id = f"rl_session_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{str(uuid.uuid4())[:8]}"
        
        self.current_session = ReinforcementSession(
            session_id=session_id,
            start_time=datetime.now(),
            end_time=None,
            config=self.config,
            status='running',
            initial_weights=self.model_weights.copy(),
            final_weights={},
            performance_metrics={}
        )
        
        logger.info(f"üöÄ –ù–∞—á–∞—Ç–∞ —Å–µ—Å—Å–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º: {session_id}")
        return session_id
    
    def end_session(self) -> Optional[ReinforcementSession]:
        """–ó–∞–≤–µ—Ä—à–∏—Ç—å —Ç–µ–∫—É—â—É—é —Å–µ—Å—Å–∏—é –æ–±—É—á–µ–Ω–∏—è"""
        if not self.current_session:
            logger.warning("‚ö†Ô∏è –ù–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–π —Å–µ—Å—Å–∏–∏ –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è")
            return None
        
        self.current_session.end_time = datetime.now()
        self.current_session.status = 'completed'
        self.current_session.final_weights = self.model_weights.copy()
        self.current_session.performance_metrics = self._calculate_session_metrics()
        
        session = self.current_session
        self.current_session = None
        
        logger.info(f"‚úÖ –°–µ—Å—Å–∏—è –æ–±—É—á–µ–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {session.session_id}")
        logger.info(f"üìà –§–∏–Ω–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞: {session.final_weights}")
        
        return session
    
    async def apply_reward(self, model_name: str, trade_pnl: float, confidence: float, trade_context: Dict[str, Any] = None) -> bool:
        """
        –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø–æ–æ—â—Ä–µ–Ω–∏—è –∑–∞ –ø—Ä–∏–±—ã–ª—å–Ω—É—é —Å–¥–µ–ª–∫—É
        
        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ AI –º–æ–¥–µ–ª–∏
            trade_pnl: –ü—Ä–∏–±—ã–ª—å/—É–±—ã—Ç–æ–∫ –æ—Ç —Å–¥–µ–ª–∫–∏
            confidence: –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –≤ —Å–∏–≥–Ω–∞–ª–µ
            trade_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç —Å–¥–µ–ª–∫–∏ (—Å–∏–º–≤–æ–ª, –≤—Ä–µ–º—è, —Ü–µ–Ω–∞ –∏ —Ç.–¥.)
        """
        if model_name not in self.model_weights:
            logger.warning(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å: {model_name}")
            return False
        
        if trade_pnl <= 0:
            logger.warning(f"‚ö†Ô∏è –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–æ—â—Ä–µ–Ω–∏—è –∑–∞ —É–±—ã—Ç–æ—á–Ω—É—é —Å–¥–µ–ª–∫—É: {trade_pnl}")
            return False
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –ø–æ–æ—â—Ä–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–±—ã–ª–∏ –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        reward_factor = self._calculate_reward_factor(trade_pnl, confidence)
        old_weight = self.model_weights[model_name]
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–µ—Å –º–æ–¥–µ–ª–∏
        weight_increase = self.config.learning_rate * reward_factor * self.config.reward_multiplier
        new_weight = min(old_weight + weight_increase, self.config.max_weight)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ
        self.model_weights[model_name] = new_weight
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ—Å–∞
        await self.normalize_weights()
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ
        change = WeightChange(
            change_id=str(uuid.uuid4()),
            session_id=self.current_session.session_id if self.current_session else "no_session",
            model_id=model_name,
            old_weight=old_weight,
            new_weight=self.model_weights[model_name],
            timestamp=datetime.now(),
            reason=f"reward_pnl_{trade_pnl:.4f}_conf_{confidence*100:.1f}%",
            trade_context=trade_context or {}
        )
        self.weight_history.append(change)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.model_performance[model_name]['wins'] += 1
        self.model_performance[model_name]['total_pnl'] += trade_pnl
        self.model_performance[model_name]['trades'].append({
            'pnl': trade_pnl,
            'confidence': confidence,
            'timestamp': datetime.now(),
            'type': 'win'
        })
        
        logger.info(f"üéâ –ü–æ–æ—â—Ä–µ–Ω–∏–µ –¥–ª—è {model_name}: {old_weight:.4f} ‚Üí {self.model_weights[model_name]:.4f} (PnL: {trade_pnl:.4f}, Conf: {confidence*100:.1f}%)")
        
        return True
    
    async def apply_punishment(self, model_name: str, trade_pnl: float, confidence: float, trade_context: Dict[str, Any] = None) -> bool:
        """
        –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–∞–∫–∞–∑–∞–Ω–∏—è –∑–∞ —É–±—ã—Ç–æ—á–Ω—É—é —Å–¥–µ–ª–∫—É
        
        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ AI –º–æ–¥–µ–ª–∏
            trade_pnl: –ü—Ä–∏–±—ã–ª—å/—É–±—ã—Ç–æ–∫ –æ—Ç —Å–¥–µ–ª–∫–∏
            confidence: –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –≤ —Å–∏–≥–Ω–∞–ª–µ
            trade_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç —Å–¥–µ–ª–∫–∏ (—Å–∏–º–≤–æ–ª, –≤—Ä–µ–º—è, —Ü–µ–Ω–∞ –∏ —Ç.–¥.)
        """
        if model_name not in self.model_weights:
            logger.warning(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å: {model_name}")
            return False
        
        if trade_pnl >= 0:
            logger.warning(f"‚ö†Ô∏è –ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–∫–∞–∑–∞–Ω–∏—è –∑–∞ –ø—Ä–∏–±—ã–ª—å–Ω—É—é —Å–¥–µ–ª–∫—É: {trade_pnl}")
            return False
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –Ω–∞–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —É–±—ã—Ç–∫–∞ –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        punishment_factor = self._calculate_punishment_factor(trade_pnl, confidence)
        old_weight = self.model_weights[model_name]
        
        # –£–º–µ–Ω—å—à–∞–µ–º –≤–µ—Å –º–æ–¥–µ–ª–∏
        weight_decrease = self.config.learning_rate * punishment_factor * self.config.punishment_multiplier
        new_weight = max(old_weight - weight_decrease, self.config.min_weight)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ
        self.model_weights[model_name] = new_weight
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ—Å–∞
        await self.normalize_weights()
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ
        change = WeightChange(
            change_id=str(uuid.uuid4()),
            session_id=self.current_session.session_id if self.current_session else "no_session",
            model_id=model_name,
            old_weight=old_weight,
            new_weight=self.model_weights[model_name],
            timestamp=datetime.now(),
            reason=f"punishment_pnl_{trade_pnl:.4f}_conf_{confidence*100:.1f}%",
            trade_context=trade_context or {}
        )
        self.weight_history.append(change)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.model_performance[model_name]['losses'] += 1
        self.model_performance[model_name]['total_pnl'] += trade_pnl
        self.model_performance[model_name]['trades'].append({
            'pnl': trade_pnl,
            'confidence': confidence,
            'timestamp': datetime.now(),
            'type': 'loss'
        })
        
        logger.info(f"üí• –ù–∞–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è {model_name}: {old_weight:.4f} ‚Üí {self.model_weights[model_name]:.4f} (PnL: {trade_pnl:.4f}, Conf: {confidence*100:.1f}%)")
        
        return True
    
    async def normalize_weights(self):
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ —á—Ç–æ–±—ã —Å—É–º–º–∞ —Ä–∞–≤–Ω—è–ª–∞—Å—å 1.0"""
        total_weight = sum(self.model_weights.values())
        
        if total_weight == 0:
            # –ï—Å–ª–∏ –≤—Å–µ –≤–µ—Å–∞ —Å—Ç–∞–ª–∏ 0, –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
            logger.warning("‚ö†Ô∏è –í—Å–µ –≤–µ—Å–∞ —Å—Ç–∞–ª–∏ 0, –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ")
            for model in self.model_weights:
                self.model_weights[model] = 1.0 / len(self.model_weights)
        elif total_weight != 1.0:
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ—Å–∞
            for model in self.model_weights:
                self.model_weights[model] = self.model_weights[model] / total_weight
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º weight decay –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        await self._apply_weight_decay()
    
    async def _apply_weight_decay(self):
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ weight decay –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"""
        if self.config.weight_decay > 0:
            for model in self.model_weights:
                self.model_weights[model] *= (1 - self.config.weight_decay)
            
            # –ü–µ—Ä–µ–Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–æ—Å–ª–µ decay
            total_weight = sum(self.model_weights.values())
            if total_weight > 0:
                for model in self.model_weights:
                    self.model_weights[model] = self.model_weights[model] / total_weight
    
    def _calculate_reward_factor(self, trade_pnl: float, confidence: float) -> float:
        """–†–∞—Å—á–µ—Ç —Ñ–∞–∫—Ç–æ—Ä–∞ –ø–æ–æ—â—Ä–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–±—ã–ª–∏ –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"""
        # –ë–∞–∑–æ–≤—ã–π —Ñ–∞–∫—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–±—ã–ª–∏ (–ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è —à–∫–∞–ª–∞)
        pnl_factor = np.log1p(abs(trade_pnl) * 100)  # log1p –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        
        # –§–∞–∫—Ç–æ—Ä —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å)
        confidence_factor = confidence ** 2
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–∞–∫—Ç–æ—Ä
        reward_factor = pnl_factor * confidence_factor
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ñ–∞–∫—Ç–æ—Ä
        return min(reward_factor, 5.0)
    
    def _calculate_punishment_factor(self, trade_pnl: float, confidence: float) -> float:
        """–†–∞—Å—á–µ—Ç —Ñ–∞–∫—Ç–æ—Ä–∞ –Ω–∞–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —É–±—ã—Ç–∫–∞ –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"""
        # –ë–∞–∑–æ–≤—ã–π —Ñ–∞–∫—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ —É–±—ã—Ç–∫–∞ (–ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è —à–∫–∞–ª–∞)
        pnl_factor = np.log1p(abs(trade_pnl) * 100)  # log1p –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        
        # –§–∞–∫—Ç–æ—Ä —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (–ª–∏–Ω–µ–π–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –¥–ª—è –Ω–∞–∫–∞–∑–∞–Ω–∏—è)
        confidence_factor = confidence
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–∞–∫—Ç–æ—Ä (–±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –Ω–∞–∫–∞–∑–∞–Ω–∏–µ –∑–∞ –≤—ã—Å–æ–∫—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Å–∏–≥–Ω–∞–ª–µ)
        punishment_factor = pnl_factor * (1 + confidence_factor)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ñ–∞–∫—Ç–æ—Ä
        return min(punishment_factor, 3.0)
    
    def _calculate_session_metrics(self) -> Dict[str, Any]:
        """–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–π —Å–µ—Å—Å–∏–∏"""
        metrics = {}
        
        for model_name, performance in self.model_performance.items():
            total_trades = performance['wins'] + performance['losses']
            win_rate = performance['wins'] / total_trades if total_trades > 0 else 0.0
            
            metrics[model_name] = {
                'total_trades': total_trades,
                'wins': performance['wins'],
                'losses': performance['losses'],
                'win_rate': win_rate,
                'total_pnl': performance['total_pnl'],
                'avg_pnl': performance['total_pnl'] / total_trades if total_trades > 0 else 0.0,
                'weight_change': self.model_weights[model_name] - self.current_session.initial_weights[model_name]
            }
        
        # –û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        total_trades = sum(m['total_trades'] for m in metrics.values())
        total_wins = sum(m['wins'] for m in metrics.values())
        total_pnl = sum(m['total_pnl'] for m in metrics.values())
        
        metrics['overall'] = {
            'total_trades': total_trades,
            'total_wins': total_wins,
            'overall_win_rate': total_wins / total_trades if total_trades > 0 else 0.0,
            'total_pnl': total_pnl,
            'avg_pnl': total_pnl / total_trades if total_trades > 0 else 0.0,
            'session_duration': (datetime.now() - self.current_session.start_time).total_seconds() / 3600  # –≤ —á–∞—Å–∞—Ö
        }
        
        return metrics
    
    def get_model_weights(self) -> Dict[str, float]:
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–µ –≤–µ—Å–∞ –º–æ–¥–µ–ª–µ–π"""
        return self.model_weights.copy()
    
    def get_current_weights(self) -> Dict[str, float]:
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–µ –≤–µ—Å–∞ –º–æ–¥–µ–ª–µ–π (–∞–ª–∏–∞—Å –¥–ª—è get_model_weights)"""
        return self.get_model_weights()
    
    def set_model_weights(self, weights: Dict[str, float]) -> bool:
        """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤–µ—Å–∞ –º–æ–¥–µ–ª–µ–π"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –≤–µ—Å–æ–≤
            if not all(model in weights for model in self.model_weights.keys()):
                logger.error("‚ùå –ù–µ–ø–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–µ–π")
                return False
            
            if not all(0 <= weight <= 1 for weight in weights.values()):
                logger.error("‚ùå –í–µ—Å–∞ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0, 1]")
                return False
            
            total_weight = sum(weights.values())
            if abs(total_weight - 1.0) > 0.001:
                logger.warning(f"‚ö†Ô∏è –°—É–º–º–∞ –≤–µ—Å–æ–≤ –Ω–µ —Ä–∞–≤–Ω–∞ 1.0: {total_weight}, –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º")
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ—Å–∞
                for model in weights:
                    weights[model] = weights[model] / total_weight
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ–≤—ã–µ –≤–µ—Å–∞
            old_weights = self.model_weights.copy()
            self.model_weights = weights.copy()
            
            logger.info(f"‚úÖ –í–µ—Å–∞ –º–æ–¥–µ–ª–µ–π –æ–±–Ω–æ–≤–ª–µ–Ω—ã: {old_weights} ‚Üí {self.model_weights}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –≤–µ—Å–æ–≤: {e}")
            return False
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–≤–æ–¥–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        summary = {
            'current_weights': self.model_weights.copy(),
            'model_performance': {},
            'weight_changes_count': len(self.weight_history),
            'session_info': {
                'active': self.current_session is not None,
                'session_id': self.current_session.session_id if self.current_session else None,
                'start_time': self.current_session.start_time if self.current_session else None
            }
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
        for model_name, performance in self.model_performance.items():
            total_trades = performance['wins'] + performance['losses']
            win_rate = performance['wins'] / total_trades if total_trades > 0 else 0.0
            
            summary['model_performance'][model_name] = {
                'total_trades': total_trades,
                'win_rate': win_rate,
                'total_pnl': performance['total_pnl'],
                'current_weight': self.model_weights[model_name]
            }
        
        return summary
    
    def reset_performance_metrics(self):
        """–°–±—Ä–æ—Å –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        for model in self.model_performance:
            self.model_performance[model] = {'wins': 0, 'losses': 0, 'total_pnl': 0.0, 'trades': []}
        
        logger.info("üîÑ –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–±—Ä–æ—à–µ–Ω—ã")
    
    def export_session_data(self, session: ReinforcementSession) -> Dict[str, Any]:
        """–≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö —Å–µ—Å—Å–∏–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è"""
        return {
            'session': asdict(session),
            'weight_history': [asdict(change) for change in self.weight_history],
            'model_performance': self.model_performance.copy()
        }